<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[《Neural Networks and Deep Learning》学习总结]]></title>
    <url>%2F2019%2F08%2F04%2F%E3%80%8ANeural%20Networks%20and%20Deep%20Learning%E3%80%8B%20%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[近日读了Michael Nielsen 的 Neural Networks and Deep Learning 这本书，之所以选择这本书来学习，是因为这本书在篇幅上比较短，同时，其对于神经网络和深度学习这块做出了整体性的概述，对于我这样的AI小白来说，先了解整体比关注细节更为重要，因此我选择了这本书作为入门资料。 我从以下几个方面总结了这本书：神经网络的相关概念、利用神经网络识别手写数字Demo演示、深度学习、优化神经网络的策略。 神经网络什么是神经网络？百度百科是这样说的：人工神经网络（Artificial Neural Networks，简写为ANNs）也简称为神经网络（NNs）或称作连接模型（Connection Model），它是一种模仿动物神经网络行为特征，进行分布式并行信息处理的算法数学模型。这种网络依靠系统的复杂程度，通过调整内部大量节点之间相互连接的关系，从而达到处理信息的目的。在工程与学术界也常直接简称为神经网络或类神经网络。 听起来略复杂，通俗总结起来的几个要点是：(1)它是一种数学模型。（2）该模型可以进行信息处理。（3）该模型具有与人来神经网络相似的结构。 感知机那么接下来，让我们通过其在学术上的相关概念来看下什么是神经网络，在介绍神经网络之前，让我们先来了解下感知机（学术上是这样教的，自我理解其跟神经元没什么太大的区别）。 我们先来看个例子： 想象一下，周末快要来了，你听说城市里将举办一个奶酪节。你挺喜欢奶酪的，所以你开始考虑要不要去参加。你发现有三个因素影响着你是否参加这个奶酪节：1.天气2.你的对象会不会陪你？（假设你有对象–单身狗表示听了很开心 hhhhh）3.奶酪节举办的地点离地铁或公交近不近?(你没车) 那么如何将该问题转化为数学模型呢？把这三个因素当作三个二元变量，x1,x2,x3当x1时，代表天气很好，而x1=0时，代表天气很坏。相同的，x2=1代表你对象愿意陪你去，x2=0为不愿意。x3同理。它们对应权重为w1 w2 w3。假如你特别特别特别喜欢吃奶酪，所以尽管对象不陪你去，交通也很偏远，你也要去参加这个奶酪节。此时，你可以设置:天气权重w1=6，对象权重w2=2 交通权重w3=2然后设置阈值为5，此时计算16*1+2*0+2*0=6&gt;5 所以，只要天气好，那么你就一定去。 我们将其用表达式表示为：图形化模型表示为： 上边的这张模型图，我们便把它叫做感知机。简化上边的公式，将其变为向量乘积的形式（其中b≡−threshold）：这就是感知机模型，由众多感知机组成的网络，则称为感知机网络。下图展示的是感知机网络完成与非门的功能：但是实际上，我们使用的神经网络用的并不是感知机模型，是因为其具有一定的局限性：如果我们想要我们的网络具备一定的学习功能，在训练网络的时候（训练网络即通过大量的数据输入，使得网络找到最合适的权值w和阈值b），那么我们就要求在调整权重w和阈值b的时候，只会导致输出output的微小变化，而感知机模型的输出只有0和1两种情况，这就导致了我们的感知机网络经常不按常理出牌。权重和 bias 的微小改变，有时候会造成非常大的影响，例如从输出0变为输出1。 这时候我们就需要一种新的模型来取代感知机–sigmoid神经元。 sigmoid神经元sigmoid神经元在整体上与感知机差别不大，主要的差异为：（1）它的输入可以是0和1之间的任何值；（2）它的输出也不止是0或者1，而是σ(w⋅x+b)，σ叫做sigmoid函数，其值为0～1之间的任何值，其表达式如下（其中z=wx+b）：其函数图像为：该神经元很好地满足了权重和阈值的微小变化对于输出的影响也是极其微小的：我们可以使用数学上偏微分证明：其实，我们可以看出感知机只是sigmoid神经元的一种特殊情况，当输出。 神经网络结构在介绍了感知机和神经元之后，现在我们可以轻松地知道神经网络是什么：即由众多神经元相互连接构成的能处理众多信息完成一定功能的一种数学模型（这里是我自己通俗的总结，大家轻喷）。下面是一张神经网络图，我们通过这张图来看一下神经网络的结构：从图中可以看到神经网络一般分为三层：输入层、隐藏层、输出层。例如：例如一张64x64的灰度图输入：4,096=64×64 个输入神经元，每个输入的范围大致从0到1隐藏层：根据网络特性不同，可以有不同的隐藏层（所以神经网络之间的不同，主要表现在不同的隐藏层，依然是我自己的理解）输出：只有一个神经元，如果其输出值小于0.5，则代表图片中的数字不是9，如果其输出值大于0.5，则代表图片中的数字是9 同样的，神经网络也具有一定的分类：一般地，可以分为这两类：（1）前馈神经网络每一层的输出当作下一层的输入，这种网络又被称为 前馈 神经网络。（2）循环神经网络其他的人工神经网络允许循环存在，这样的模型叫做循环神经网络，循环神经网络一般用在语音识别等方面。 梯度下降算法我们在利用神经网络解决现实问题的时候，第一步往往是构建上边所讲的神经网络，第二步则是寻找合适的数据集进行训练。在进行训练（学习）的时候，我们需要一个算法，它能够使用数据进行学习，从而帮助我们找到一组合适的阈值（b）和权值（w），这个算法就是梯度下降算法。 该算法的核心思想是：使权值和阈值朝着代价降低的方向变化，代价变为最低时的权值和阈值即为我们要找的权值和阈值。因此该算法需要一个代价函数，来评估学习的效果，这里我们的代价函数选用二次代价函数作为我们的代价函数：其中，y为期望输出，a为输入x后实际输出的输出向量，如果我们能找到一组权值和 biases，使得 C(w,b)≈0，就代表着我们的算法非常有效。相反，如果 C(w,b)的值很大，说明网络的输出 y(x)距离真实的 a 非常远。那么有了代价函数之后，梯度下降算法的最终目标即为：找到这样一组权值和biases，使得代价 C(w,b) 最小化。我们来看下梯度下降算法具体是怎样执行的：我们来看这样一个图形，这就是我们画出来的代价函数的大致的一个图形，我们的目标就是找到处在谷底的那个点，那么怎样进行呢，我们需要用数学表达式来表示：这个图中某一点的梯度可以表示为：向量形式为：这几个式子涉及到高数里边的偏微分的知识，看不懂的可以稍微复习下高数，那么我们如何确定Δv，使得代价函数C的值一直变小，即ΔC&lt;0，即朝着谷底的方向移动。为什么呢？证明如下：其中η我们称之为学习率，那么η值又该如何确定呢，在本书中给出了一个参考，但是这不是适用于所有的情况，要根据实际情况确定：至于为什么这样取值可以满足需要，我给出了如下的证明： 反向传播算法在利用梯度下降算法进行学习时，计算梯度也是其中关键的一步，计算梯度的算法：反向传播算法这四个算法的具体含义：BP1是根据L层的z来计算L层的误差。BP2是根据L+1层的误差来估算第L层的误差。BP3则是由BP1计算出的误差计算，梯度的的横坐标（姑且允许我这样叫），BP4则是计算出来梯度的纵坐标。 对于这几个式子的证明，网上已经有许多人证明过，感兴趣的大家可以自己去搜一下。 构建神经网络识别手写数字的DEMO在这本书中，给出了一个神经网络的demo，这个demo的核心代码仅仅有70多行，便可以达到百分之九十五以上的识别准确率。其核心代码，network.py我贴在下边：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141&quot;&quot;&quot;network.py~~~~~~~~~~A module to implement the stochastic gradient descent learningalgorithm for a feedforward neural network. Gradients are calculatedusing backpropagation. Note that I have focused on making the codesimple, easily readable, and easily modifiable. It is not optimized,and omits many desirable features.&quot;&quot;&quot;#### Libraries# Standard libraryimport random# Third-party librariesimport numpy as npclass Network(object): def __init__(self, sizes): &quot;&quot;&quot;The list ``sizes`` contains the number of neurons in the respective layers of the network. For example, if the list was [2, 3, 1] then it would be a three-layer network, with the first layer containing 2 neurons, the second layer 3 neurons, and the third layer 1 neuron. The biases and weights for the network are initialized randomly, using a Gaussian distribution with mean 0, and variance 1. Note that the first layer is assumed to be an input layer, and by convention we won&apos;t set any biases for those neurons, since biases are only ever used in computing the outputs from later layers.&quot;&quot;&quot; self.num_layers = len(sizes) self.sizes = sizes self.biases = [np.random.randn(y, 1) for y in sizes[1:]] self.weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])] def feedforward(self, a): &quot;&quot;&quot;Return the output of the network if ``a`` is input.&quot;&quot;&quot; for b, w in zip(self.biases, self.weights): a = sigmoid(np.dot(w, a)+b) return a def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None): &quot;&quot;&quot;Train the neural network using mini-batch stochastic gradient descent. The ``training_data`` is a list of tuples ``(x, y)`` representing the training inputs and the desired outputs. The other non-optional parameters are self-explanatory. If ``test_data`` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out. This is useful for tracking progress, but slows things down substantially.&quot;&quot;&quot; if test_data: n_test = len(test_data) n = len(training_data) for j in xrange(epochs): random.shuffle(training_data) mini_batches = [ training_data[k:k+mini_batch_size] for k in xrange(0, n, mini_batch_size)] for mini_batch in mini_batches: self.update_mini_batch(mini_batch, eta) if test_data: print &quot;Epoch &#123;0&#125;: &#123;1&#125; / &#123;2&#125;&quot;.format( j, self.evaluate(test_data), n_test) else: print &quot;Epoch &#123;0&#125; complete&quot;.format(j) def update_mini_batch(self, mini_batch, eta): &quot;&quot;&quot;Update the network&apos;s weights and biases by applying gradient descent using backpropagation to a single mini batch. The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta`` is the learning rate.&quot;&quot;&quot; nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] for x, y in mini_batch: delta_nabla_b, delta_nabla_w = self.backprop(x, y) nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)] nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)] self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)] self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)] def backprop(self, x, y): &quot;&quot;&quot;Return a tuple ``(nabla_b, nabla_w)`` representing the gradient for the cost function C_x. ``nabla_b`` and ``nabla_w`` are layer-by-layer lists of numpy arrays, similar to ``self.biases`` and ``self.weights``.&quot;&quot;&quot; nabla_b = [np.zeros(b.shape) for b in self.biases] nabla_w = [np.zeros(w.shape) for w in self.weights] # feedforward activation = x activations = [x] # list to store all the activations, layer by layer zs = [] # list to store all the z vectors, layer by layer for b, w in zip(self.biases, self.weights): z = np.dot(w, activation)+b zs.append(z) activation = sigmoid(z) activations.append(activation) # backward pass delta = self.cost_derivative(activations[-1], y) * \ sigmoid_prime(zs[-1]) nabla_b[-1] = delta nabla_w[-1] = np.dot(delta, activations[-2].transpose()) # Note that the variable l in the loop below is used a little # differently to the notation in Chapter 2 of the book. Here, # l = 1 means the last layer of neurons, l = 2 is the # second-last layer, and so on. It&apos;s a renumbering of the # scheme in the book, used here to take advantage of the fact # that Python can use negative indices in lists. for l in xrange(2, self.num_layers): z = zs[-l] sp = sigmoid_prime(z) delta = np.dot(self.weights[-l+1].transpose(), delta) * sp nabla_b[-l] = delta nabla_w[-l] = np.dot(delta, activations[-l-1].transpose()) return (nabla_b, nabla_w) def evaluate(self, test_data): &quot;&quot;&quot;Return the number of test inputs for which the neural network outputs the correct result. Note that the neural network&apos;s output is assumed to be the index of whichever neuron in the final layer has the highest activation.&quot;&quot;&quot; test_results = [(np.argmax(self.feedforward(x)), y) for (x, y) in test_data] return sum(int(x == y) for (x, y) in test_results) def cost_derivative(self, output_activations, y): &quot;&quot;&quot;Return the vector of partial derivatives \partial C_x / \partial a for the output activations.&quot;&quot;&quot; return (output_activations-y)#### Miscellaneous functionsdef sigmoid(z): &quot;&quot;&quot;The sigmoid function.&quot;&quot;&quot; return 1.0/(1.0+np.exp(-z))def sigmoid_prime(z): &quot;&quot;&quot;Derivative of the sigmoid function.&quot;&quot;&quot; return sigmoid(z)*(1-sigmoid(z)) 接下来，我们进行一个实际操作。在进行实际操作前，需要交代一下，程序运行的一些基本配置信息： 运行环境：python2终端（用python3有些语法不兼容）训练数据集：MINIST数据集使用到的Python库：Numpy权重和biases的随机初始化：利用的是 Numpy 中的 np.random.randn 函数，生成期望为0，标准差为1的高斯分布 接下来我们运行一下程序：首先加载数据集：123&gt;&gt;&gt; import mnist_loader&gt;&gt;&gt; training_data, validation_data, test_data = \... mnist_loader.load_data_wrapper() 构建神经网络：784个神经元的输入层、30个神经元的隐藏层、10个神经元的输出层12&gt;&gt;&gt; import network&gt;&gt;&gt; net = network.Network([784, 30, 10]) 训练网络：训练代数：30、学习率为3.0、mini-batch的大小为101net.SGD(training_data, 30, 10, 3.0, test_data=test_data) 运行的结果：可以看到，我们的结果达到了95%左右，这是未经优化过的结果，已经很令人满意了，在这本书的后边章节，经过优化以后，准确率可以达到98%左右，我在本篇博客中便不再赘述，感兴趣的可以搜下这本书。当然你也可以，改变隐藏层的数量，以及学习率等，观察一下训练的结果将如何变化。例如：我将隐藏神经元改为100，发现训练的效果很差 深度学习由于在这本书中，对于深度学习，只谈及到了一点，所以，我在这里只谈一下自己学到的一些东西。通过数字识别的实战，我们不难发现：有时，通过增加隐藏神经元的层数可以使得训练效果变好，但是有时候却起到相反的效果，为什么？随着神经网络的层数增多，由于反向传播的特点，会出现梯度消失和梯度爆炸等问题，这些问题会导致越深层次的神经网络，越难训练。即会出现梯度消失和梯度爆炸的问题：梯度消失：对于某些深层神经网络来说，其梯度在隐藏层中进行反向传播时，会变得越来越小。这意味着前面的隐藏层中的神经元会学习地非常慢。其一般由sigmoid函数的特性引起。梯度爆炸：跟梯度消失相反，梯度爆炸问题是梯度在反向传播中呈指数级增长。那么如何解决梯度不稳定的问题呢？ 使用卷积神经网络其可以很好解决深层网络难以训练的问题，卷积层可以极大地地减少网络需要的参数，让训练变得更为容易。其主要由三步构成：局部接受域、共享权值和池化 使用更多更强大的正则化技巧（特别是 dropout 和卷积层）去减轻过拟合来使得梯度达到平稳状态 使用 ReLU 激活函数而不是 sigmoid 激活函数，其可以加快训练速度 优化神经网络的策略 交叉熵解决学习缓慢的问题将学习使用的二次代价函数使用交叉熵代价函数代替： 为什么交叉熵函数相较二次代价函数可以解决学习缓慢的问题呢？我们可以看下我们使用交叉熵算出来的梯度：使用二次函数计算出来的梯度：二者对比，我们可以发现交叉熵函数中梯度的表达式不包含sigmoid函数的导数，由于sigmoid函数的图像特性，其在两端会趋向于平缓导致学习缓慢，而交叉熵代价函数则不会。 过拟合问题即训练结果在训练数据上表现得很好，但是一旦换了其他的数据，效果变得很差，就是说训练的神经网络不具备一定的普遍性。如何解决？ 通过增加训练集解决过拟合问题简单的学习算法+好的训练集&gt;复杂的学习算法可以通过寻找质量高的训练集或者增加训练集的数量来解决过拟合问题。 正则化解决过拟合的问题L2正则化后的交叉熵代价函数为：其包含一个lamda参数，这个也需要我们根据情况设置不同的lamada参数。 Dropout解决过拟合问题dropout减轻过拟合是通过随机隐藏一些隐藏神经元然后反复执行这个过程，反复训练，最后求出的一组权值和bias。 寻找合适的超参数来解决过拟合问题超参数包括：学习率、mini-batch、隐藏层数量、正则化系数等。 3.改变其他神经元激活函数来取得更好的学习效果 使用tanth神经元激活函数 使用ReLU 激活函数 总结通过这本书的学习，虽然有些概念以及所传达的思想还未完全吃透弄懂，但是对于神经网络和深度学习有了一个整体的概念，这有助于后续进一步学习AI的相关知识，AI小白我也算是入了个门吧，此外，这篇文章只是我按照自己的思路总结出的一篇笔记，如有错误，还望大家指正。]]></content>
      <categories>
        <category>AI</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>神经网咯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git常用命令总结]]></title>
    <url>%2F2017%2F09%2F10%2Fgit%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[最近看了廖雪峰老师的git系统教程，下面对于经常会用到的git命令进行精简总结，便于以后查阅： 创建版本库如果要使用git，需要首先在文件夹中使用以下命令创建git版本库：1git init 创建成功，会在文件夹中显示一个.git的文件夹，这就是初始化的git仓库。 添加文件到git仓库添加文件到git仓库分为两步：（1）使用以下命令将文件添加到git仓库的暂存区，该命令可反复多次使用，添加多个文件： 1git add &lt;file&gt; eg:git add test.txt (2)使用以下命令将文件提交到git仓库： 1git commit -m&quot;本次修改的提示信息&quot; eg：git commit test.txt -m”test” 时光穿梭机1、两个常用命令：12git status //随时掌握工作区的状态git diff //查看修改内容 git status告诉你有没有文件被修改过，git diff 告诉你具体修改了哪些内容。eg：git diff 1.txt 2、版本的切换（1）穿梭前，用以下命令查看提交历史，获得commit_id,以便确定要回退到哪个版本。 1git log 然后再使用以下命令进行版本回退： 1git reset --hard commit_id (2)快捷方式，可以不用第一步，直接进行版本的回退：HEAD指向的版本就是当前版本，那么 HEAD^ 表示上一个版本，上上个版本是HEAD^^ ,那么上100个版本就是HEAD~100，因此，Git允许我们在版本的历史之间穿梭，使用以下命令进行版本的回退： 1234git reset --hard HEAD^或git reset --hard HEAD^^... (3)重返未来，当你版本回退之后，你发现版本回退错误，要回到刚刚那个版本，那么你就需要用以下命令找到未来的commit_id: 1git relog 该命令可以查看提交历史，以便确定要回退到哪个版本，然后在用git reset –hard commit_id穿越即可。 3、管理修改&amp;撤销修改场景1：当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令： 1git checkout --file 场景2：当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令： 1git reset HEAD file 就回到了场景1，第二步按场景1操作。 4、删除文件1git rm filename 用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容如果是想要真正删除，git rm filename之后 再commit一下就行了，但是如果你是误删的，那么git checkout – filename就可以了。 远程仓库关联远程仓库很多时候你需要关联一个远程仓库：要关联一个远程库，使用命令： 1git remote add origin git@server-name:path/repo-name.git 远程仓库默认命名为orgin，关联后，使用命令：1git push -u origin master 第一次推送master分支的所有内容；此后，每次本地提交后，只要有必要，就可以使用命令： 1git push origin master 推送最新修改； 克隆仓库要克隆一个仓库，首先必须知道仓库的地址，然后使用以下命令克隆到本地： 1git clone address Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。 分支管理创建与合并分支： 查看分支：git branch 创建分支：git branch 切换分支：git checkout 创建+切换分支：git checkout -b 合并某分支到当前分支：git merge 删除分支：git branch -d 解决冲突（1）当Git无法自动合并分支时，就必须首先解决冲突。可直接进入冲突文件查看冲突，解决冲突后，再提交，合并完成。（2）1git log --graph --pretty=oneline 该命令可以看到分支合并图。Git分支十分强大，在团队开发中应该充分应用。 合并分支时，加上–no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 1git merge --no-ff -m&quot;message&quot; branchname 其他1、修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除；当手头工作没有完成时，先把工作现场1git stash 一下，然后去修复bug，修复后，再1git stash pop 回到工作现场。 2、开发一个新feature，最好新建一个分支； 如果要丢弃一个没有被合并过的分支，可以通过git branch -D 强行删除。 3、查看远程库信息，使用1git remote -v 本地新建的分支如果不推送到远程，对其他人就是不可见的；从本地推送分支，使用1git push origin branch-name ，如果推送失败，先用1git pull 抓取远程的新提交；在本地创建和远程分支对应的分支，使用1git checkout -b branch-name origin/branch-name 本地和远程分支的名称最好一致；建立本地分支和远程分支的关联，使用1git branch --set-upstream branch-name origin/branch-name 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 标签管理123git tag &lt;name&gt;或git tag &lt;name&gt; commit_id 用于新建一个标签，默认为HEAD，也可以指定一个commit id：123git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot; commit_id或git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot; 用于指定标签信息； 123git tag -s &lt;tagname&gt; -m &quot;blablabla...&quot;或git tag -s &lt;tagname&gt; -m &quot;blablabla...&quot; commit_id 用于用PGP签名标签； 1git tag 可以查看所有标签。 1git push origin &lt;tagname&gt; 用于推送一个本地标签；1git push origin --tags 用于推送全部未推送过的本地标签；1git tag -d &lt;tagname&gt; 用以删除一个本地标签。 1git push origin :refs/tags/&lt;tagname&gt; 用以删除一个远程标签。]]></content>
      <categories>
        <category>git</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[php环境搭建]]></title>
    <url>%2F2017%2F08%2F16%2Fphp%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[1、 下载xammp（a） 最好不要下载最新的版本，因为新版本与PhpStorm等编程环境会存在不兼容问题，老是会出现一些莫名其妙的问题，所以建议下载低版本。（b） 由于xammp已经把mysql 和 apache等综合在一起了，不需要再去一个个下载，其还有控制面板，用起来方便多了。 可到其官网上下载，下载xammp 2、 配置端口下载完成后打开xammp的控制面板，点击apache和 mysql的run，观察其启动是否正常，若启动失败，一般都是因为它的端口被占用了，这时候就需要修改它的配置文件，点击其后的config按钮就可以配置了，具体的操作步骤，可以看这个人的博客：点击这里 3、 配置虚拟地址Xammp配置好以后，我们就来搭建本地虚拟地址，目的在于可以在浏览器中通过虚拟地址来查看php编程的运行结果。（a） 新建一个文件夹，这个文件夹是你要放入php文件的文件夹，也就是说只有放在该文件夹中的php文件才能通过虚拟地址访问。如下图，我在d盘的xammp文件夹下的htdoc下新建了pro文件夹，用以存放我将要写的php文件，其路径为：D:\xampp\htdocs\pro （b） 接下来就要开始搭建虚拟环境了，首先，找到你刚才下载的xammp文件夹，按照这个路径D:\xampp\apache\conf\extra打开httpd-vhosts.conf,注意：最好用notepad打开。在配置文件后加入以下字段： 123456&lt;VirtualHost *:80&gt; DocumentRoot &quot;D:/xampp/htdocs/pro&quot;//注意这里填写你刚刚新建文件夹的地址 ServerName www.mz.com //这里就填写你想要搭建的虚拟地址，根据自己的喜好，随便填写 ##ErrorLog &quot;logs/dummy-host2.example.com-error.log&quot; ##CustomLog &quot;logs/dummy-host2.example.com-access.log&quot; common&lt;/VirtualHost&gt; （c） 配置本地的hosts文件，一般在c盘中，不同电脑不同，可在文件夹搜索框中搜索hosts，找到后依然用notepad++ 打开，在该配置文件中，加入以下语句： 1127.0.0.1 www.mz.com（注意这里要和上边httpd-vhosts.conf中配置的虚拟地址相同） (d) 现在我们就可以在刚刚新建的文件夹中写php文件来观察运行结果了。 以我刚刚新建的pro文件夹为例，我在文件夹pro中新建ahello.php 我们以notepad++打开ahello.php 保存。然后在浏览器中打开，输入你刚刚配置的虚拟地址+ahello.php如我的是：http://www.mz.com/ahello.php 输出正确。按照这个步骤你就可以开始php编程练习了。]]></content>
      <categories>
        <category>php</category>
      </categories>
      <tags>
        <tag>php</tag>
        <tag>xammp</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Unity3D简单游戏演示]]></title>
    <url>%2F2016%2F10%2F28%2FUnity3D%E7%AE%80%E5%8D%95%E6%B8%B8%E6%88%8F%E6%BC%94%E7%A4%BA%2F</url>
    <content type="text"><![CDATA[首先是环境的安装： UNity3d的安装， 下载安装包点这里 破解：打开下载的unpch95_unityCrack文件夹，拷贝Unity 4.x Pro Patch.exe到unity安装目录下Editor文件夹下面，然后运行，点Patch按钮，读完条之后再点Ce Lic按钮，破解完成。 配置环境：打开破解好的unity3d，点Edit选项卡，选择Preferences…找到External Tools，会看到External Script Editor是Monodevelop，点Browser选择自己喜欢的Editor，我自己用的是vs2010，完成以后就可以创建新工程了。 ##主要完成的任务 一个是人物的移动；第二个是人物动画的添加。 人物的移动 主要是靠一个叫做player的文件来控制，点击project视窗中的script穿件c#文件，命名为player文件，其代码即注释如下： using UnityEngine; using System.Collections; public class player : MonoBehaviour { public float walkSpeed;//用来控制角色移动的速度 // Use this for initialization void Start() { walkSpeed = 5.0f; } void Update() { Vector3 direction = Input.GetAxis(&quot;Horizontal&quot;) * transform.right + Input.GetAxis(&quot;Vertical&quot;) * transform.forward; transform.position += walkSpeed * Time.deltaTime * direction;//控制角色的位置移动。 } } 将该脚本与人物关联：直接拖动player文件到player上去，此时点击player，可以看到其Inspector框最下边Add Component上边多了player的控制行，此时点击运行游戏，按上下左右键人物即可前后左右移动。 动画的添加 主要是通过另一个脚本语言，player_animato控制，其代码实现与注释如下： using UnityEngine; using System.Collections; public class player_animator : MonoBehaviour { public Animator animator; private Vector3 lastPosition; private float Speed;//移动速度的控制 private float Angle; // Use this for initialization void Start () { lastPosition = transform.position; } // Update is called once per frame void Update () { Vector3 Velocity = (transform.position - lastPosition) / Time.deltaTime;//下一帧移动位置的确定 Vector3 localVelocity = transform.InverseTransformDirection(Velocity);//自身坐标系的三维变量 localVelocity.y = 0; Speed = localVelocity.magnitude; Angle = (HorizontalAngle(localVelocity)+360f)%360; animator.SetFloat(&quot;Speed&quot;,Speed); animator.SetFloat(&quot;Angle&quot;,Angle); lastPosition = transform.position; } float HorizontalAngle(Vector3 localVelocity) { float angle = Mathf.Atan2(localVelocity.z, localVelocity.x) * Mathf.Rad2Deg;//反正切函数，第一个参数为对边，第二个为邻边 return angle; } } 具体Animator的控制，点击这里 遇到的问题 在Animator中，添加Speed，变量的时候，写成了speed与前边的脚本文件不一致，导致动画效果出不来。 感悟与体会 这次培训，整体上感觉蛮好玩的，因为通过自己的编写操作，人物就自己动起来了，以前只是玩游戏，现在真正地让自己去设置的时候，感觉还蛮新奇好玩的，但是由于自己以后可能不会从事游戏这个方向，所以课下也就没有花很多时间去学习它，bullet那一部分也没有自己去钻研，但是，这次的培训让我了解了一下简单游戏的诞生过程，还是蛮开心的。]]></content>
      <categories>
        <category>Unity3D</category>
        <category>workshop</category>
      </categories>
      <tags>
        <tag>Unity3D</tag>
        <tag>Game</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python爬虫之教务处成绩]]></title>
    <url>%2F2016%2F10%2F22%2Fpython%E7%88%AC%E8%99%AB%E4%B9%8B%E6%95%99%E5%8A%A1%E5%A4%84%E6%88%90%E7%BB%A9%2F</url>
    <content type="text"><![CDATA[学习过程 这次作业主要是python爬虫，由于之前并没有接触过这个东西，所以我先到网上简单地学习了一下python语言，具体的学习网站python基础教程-菜鸟教程 操作部分 安装环境：pycharm编程环境+anaconda包集+selenium库+phantomJS无界面爬虫浏览器 在pycharm中新建.py文件开始编程。 导入部分： from selenium import webdriver #导入部分 from bs4 import BeautifulSoup import xlwt#制成excel表格的一个库 import string phantomjs_path=&apos;C:\Users\mazhuang\Desktop\Workshop\phantomjs-2.1.1-windows/bin/phantomjs.exe&apos; #加载浏览器驱动，无界面爬虫浏览器在电脑中的路径 driver = webdriver.PhantomJS(phantomjs_path) 打开网页： URL = &apos;目标网址&apos; #网页地址，即你所要抓取数据的网页 driver.get(URL)#返回网页地址 driver.implicitly_wait(10)#设置隐式等待时长 模拟登陆： 首先，要进行元素的定位。 单个元素定位： find_element_by_id find_element_by_name find_element_by_xpath find_element_by_link_text find_element_by_partial_link_text find_element_by_tag_name find_element_by_class_name find_element_by_css_selector 多个元素定位： find_elements_by_name find_elements_by_xpath find_elements_by_link_text find_elements_by_partial_link_text find_elements_by_tag_name find_elements_by_class_name find_elements_by_css_selector 其中，id，name，xpath等都是要从网页那里通过检查元素的得到的标签。比如说，要模拟登陆首先要进行网页上学号和密码输入框的定位，此时，用浏览器打开目标网页，右键单击要输入学号的地方，检查源，得到其id，name等信息在运用上边提到的函数进行元素定位。 页面的交互，我主要用到了这两个函数： click()//模拟鼠标的左键点击 send_key(value)//类似于键盘传输值的信息 submit()//模拟提交 如：要进行学号的输入： driver.find_element_by_id(&quot;textfield&quot;).send_keys(&apos;学号&apos;)//textfiled即为通过定位元素扎到的标签，用send_key(value)将学号信息传输过去 要进行历史成绩查询的点击： driver.find_element_by_link_text(&apos;历史成绩查询&apos;).click()//这里主要演示click的用法，submit()用法类似 用BeautifulSoup来抓取解析自己所需要的信息： lscj = driver.find_element_by_id(&apos;undefined&apos;).get_attribute(&quot;innerHTML&quot;)#找到要抓取的模块 Soup = BeautifulSoup(lscj, &apos;lxml&apos;) 可以用print Soup pretiffy()来运行一下看是不是自己需要的信息。 将抓取到的信息写进excel表格 首先要引入xlwt库 import xlwt excel的写入 exl = xlwt.Workbook(encoding=&apos;utf-8&apos;,style_compression=0)//创建用于写excel的exl对象其中encoding=‘utf-8’是防止解析出来的中文乱码 sheet = exl.add_sheet(&apos;mazh&apos;, cell_overwrite_ok=True)//创建表单，并且声明为可以覆盖 然后，通过sheet.write(列，行，内容)语句将所抓取的数据写进去。在进行数据写入的时候需要先找到要录入表格元素的标签，然后通过循环语句将要写入的数据写进表格。找到元素标签，主要运用这个语句： Soup.find_all(&apos;标签名称&apos;) 用 变量.string 来获取该处的内容 别忘了用.decode(‘utf-8’)来解码。 最后保存 exl.save(&quot;文件名.xls&quot;) 遇到的问题： (1)一开始，对一些定位元素的函数不会用，于是我就搜了一些运用这些函数的实例，这个实例运用推荐给大家定位元素算法的实例运用 (2)对将信息写入excel表格毫无头绪，于是就搜了一下这个网站python爬虫excel的读写 (3)一开始所保存的表格出来是乱码的，在问了同学之后，用decode(‘utf-8’)先解码再打印就好了。 还未解决的问题 由于我的成绩有22条，而页面上显示了20条，需要将页面显示数设置成50，我尝试用了各种方法，都没有成功，虽然并没有报错，但是页面效果并无反应。关于下拉框选择我尝试的方法如下网页1网页2 感悟及体会 之前没有接触过python，所以对于很多东西都无从下手，但是在自我摸索的过程中，虽然很费时费力，但是真正自学到东西还是蛮开心的，再者，就是对于自己一些连方法都看不懂的语句，要多找一些例子来看一下，在例子中更能理解其如何运用。等自己真正把这件事做出来的时候，就会觉得，其实也没什么难得。 最后十分感谢，在我自己实在摸索不出来的时候，肖同学对我的帮助，在这里推荐一下她的博客，博客]]></content>
      <categories>
        <category>python</category>
        <category>selenium</category>
        <category>workshop</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>selenium</tag>
        <tag>webdriver</tag>
        <tag>phantomjs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[淘宝登录界面的编写]]></title>
    <url>%2F2016%2F10%2F14%2F%E6%B7%98%E5%AE%9D%E7%99%BB%E5%BD%95%E7%95%8C%E9%9D%A2%E7%9A%84%E7%BC%96%E5%86%99%2F</url>
    <content type="text"><![CDATA[#第二次培训感悟 ##课上学习 本次课程得主要任务是完成淘宝登录页面的编写，课上学长带领我们主要完成了淘宝登录页面的主要框架的搭建，主要是.html和.css的编写。课上遇到的主要问题是，以前由于没有接触过html和css，所以刚开始对学长所讲的东西听不懂，只是简单地跟着学长抄写学长在大屏幕上写的代码，但是后来通过自己与图片的对比，渐渐了解了每一个语句它的功能，虽然说整个过程有点懵，但是最后看到那个页面的雏形，还是蛮开心的。 ##课下学习 主要遇到的问题主要是在javascript部分以及界面设计的部分，主要问题如下： （1）首先，是输入框中背景提示文字的设置。由于不知道从哪里下手，于是去网上搜了一下，得到了如下的解决方案： input type=”text” class=”form-control” placeholder=”账户（手机号)” id=”username” input type=”password” class=”form-control” placeholder=”密码” name=”password1” id=”password” 即如上面所示，在相应的输入输出框模块中添加语句placeholder=”账户（手机号)” 和placeholder=”密码”即可，需要说明的是placeholder 属性是 HTML5 中的新属性。placeholder 属性提供可描述输入字段预期值的提示信息（hint）。该提示会在输入字段为空时显示，并会在字段获得焦点时消失。 注释：placeholder 属性适用于以下的&lt;input&gt; 类型：text, search, url, telephone, email 以及 password。 效果如下 （2）对js完全没有接触过，不知道从哪下手。于是在网易云课堂上听了HTML+css+javascript的基础章节，对js有了初步的了解。我觉得讲的挺好的，网易云课堂html+css+js. （3）js主要控制语句的编写。由于是输入不同情况，出现不同的警告语句以及提示语句，所以采用if的控制语句，具体做法如下： if (username.value == &quot;&quot;) {//实现语句} if (!num.test(username.value)) {//实现语句} if (password.value == &quot;&quot;) {//实现语句} if (username.value !=&quot;13012345678&quot;) {//实现语句} if (password.value!= 123456) {//实现语句} else{//实现语句} （4）警告框与提示框的设置。（3）中的语句编写成功后发现，弹出的警告框如下图所示： 于是，查了一下，警告框的设置主要是采用了bookstrap的样式，在html中的具体语句如下（这里仅以其中一种警告框为例，其他的类似）： &lt;div class=&quot;alert alert-danger&quot; style=&quot;display: none; height: 27px; margin-right: 20px; margin-left: 20px; padding: 0px&quot;id=&quot;alert_warn1&quot;&gt; &lt;img src=&quot;error.png&quot; alt=&quot;warn&quot; width=&quot;22px&quot; height=&quot;22px&quot; style=&quot;color: white&quot;&gt; &lt;b&gt; style=&quot;color: #f84e4e; font-size: smaller; font-weight: 700;font-size: 15px;font-family: &apos;Microsoft YaHei UI&apos;&quot;&gt;用户名不能为空!&lt;/b&gt; &lt;/div&gt; 上边的style=”display: none;语句表示警告框的显示形式为隐藏，也就是在html界面其实看不到的。 （5）控制警告框的出现。具体实现语句如下： var warn1 = document.getElementById(&quot;alert_warn1&quot;); if (username.value == &quot;&quot;) { warn1.style.display=&quot;block&quot;;//（m） warn2.style.display=&quot;none&quot;; warn3.style.display=&quot;none&quot;; warn4.style.display=&quot;none&quot;; warn5.style.display=&quot;none&quot;; success.style.display=&quot;none&quot;; setTimeout(&quot;codefans1()&quot;,3000);//3秒 username.value.focus(); return false; } 上边的m语句即表示显示该警告框而隐藏其他警告框。 （6）将编辑好的js文件用于html文件中，主要是运用该语句 &lt;script src=&quot;js.js&quot; type=&quot;text/javascript&quot;&gt; &lt;/script&gt; 而何时用js中编写好的控制语句则用该语句： &lt;button class=&quot;btn&quot; type=&quot;button&quot; onclick=&quot;setTimeout($check(), 200);&quot; id=&quot;buttons&quot;&gt;登&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;录&lt;/button&gt; 主要是其中的onclick那一句。 （7）关于点击登录按钮0.2秒后出现警告框以及3秒后消失是这样设置的： 0.2秒后出现警告框： onclick=&quot;setTimeout($check(), 200);通过setTime函数实现。 3秒后消失： if (username.value == &quot;&quot;) { warn1.style.display=&quot;block&quot;; warn2.style.display=&quot;none&quot;; warn3.style.display=&quot;none&quot;; warn4.style.display=&quot;none&quot;; warn5.style.display=&quot;none&quot;; success.style.display=&quot;none&quot;; setTimeout(&quot;codefans1()&quot;,3000);//3秒后消失 username.value.focus(); return false; } function codefans1(){ var warn1 = document.getElementById(&quot;alert_warn1&quot;); warn1.style.display=&quot;none&quot;; }//控制消失的函数实现 （8）在设置输入手机号必须是11位数字时，我用了该语句：if(username.toString.length!=11)发现通过不了验证，就在群邮里问一下，原来username得到的不是输入框里面的信息，是通过usernme.value取出来的，而且toString 方法输出的是对应对象的信息：[object HTMLInputElement],所以这里是不对的。 ##结果展示 如下面图片所示：]]></content>
      <categories>
        <category>html</category>
        <category>css</category>
        <category>javascript</category>
      </categories>
      <tags>
        <tag>html</tag>
        <tag>css</tag>
        <tag>javascript</tag>
        <tag>workshop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的第一次培训]]></title>
    <url>%2F2016%2F10%2F05%2F%E6%88%91%E7%9A%84%E7%AC%AC%E4%B8%80%E6%AC%A1%E5%9F%B9%E8%AE%AD%2F</url>
    <content type="text"><![CDATA[Workshop第一次培训感悟体会 背景 上周日参加了学长组织的Workshop的第一次培训，第一次培训的内容是静态博客的搭建，给我们授课的是姜建林学长，在这次培训中，我遇到了许多问题，但同时也学习到了许多： 课上学习 首先，学长给我们讲述了为什么要搭建静态博客，通过几个例子，我们便很快体会到了静态博客的优点：个人的学习心得可以记录下来，以方便日后查看，同时可以方便别人观看自己的成果，自由开放的空间，可以毫无拘束的发表自己的看法，而且如果有一个属于自己域名是一件非常炫酷以及自豪的事情，想到这些，我就变得兴奋起来。 主要学到的东西：线下我们主要完成了Github的注册，ssh密钥和公钥的获取并将其复制在Github的ssh-key中，以及Hexo配置的其中一部分。遇到的问题： （1）如上图所示，在ssh配置邮箱时，要执行ssh-keygen -t rsa -C1625128973@qq.com时，报错，报错语句为：-keygen-t： command not found ，反复输了两次依旧报错，后来经过自己的查找，发现自己的命令输入出错，在-keygen 和 -t之间少了一个空格。这就告诉我在执行命令语句时一定要细心。 （2）在安装hexo时，当用Gitbash执行npm install hexo -g 之后发现页面停留在那个页面好长时间没有回应，后来，学长提示我们说可能是安装的Node.js的版本跟电脑的系统不匹配，于是我卸载了原来的Node.js，重新到官网上下载了安装包并进行安装，重新尝试，发现成功了。 （3）之后，在进行hexo的初始化时，执行hexo init 时，发现程序又报错，于是我就去重新配置了一下Node的环境变量，重新添加了Node的Path，之后重新尝试，终于成功。下图为线下学习的部分截图： 图为克隆远程仓库和提交申请成功后github的页面 图为在浏览器中输入mz233.github.io的404界面 图为hexo配置成功后，在浏览器中输入Localhost：4000后显示的界面 课下学习 主要完成的任务： 将Hexo与远程Github关联，主题的设置，多说评论的设置。 遇到的问题: 图一 图二 （1）如上图一，在执行hexo deploy命令时报错，后来重新查看了congyml文件，发现出现了图二的错误，正确的应该是： deploy： type： git reposity： git@github.com:mz233/mz233.github.io.git branch： master 而我粗心地写成了如图所示的图样。 图三 图四 图五 （2）在做步骤一的修改后，执行hexo deploy出现了图三的情况，后来看了邮件里的评论，是因为在config.yml文件配置时deploy下面的三个变量配置内容时要在其前边加一个空格，重新配置执行hexo d，hexo g之后，成功，如图四所示。 （3）图片问题，刚开始将本地图片的绝对路径弄上去之后，发现预览界面图片显示不出来，后来又尝试了将图片上传到百度图片生成图片外链，发现依旧不能成功，出现如图五的情景，于是我又到知乎上边搜了一下，找到了一个图床，将图片上传可以生成图片外链，在这里分享给大家，很好用。图床点击这里 我的意外收获 （1）知道了mkdir是新建文件夹的命令；（2）上课时学长提到了DNS，课下了解了什么是DNS；（3）了解了一下什么是URL。 我的感悟： 在这次培训中，我的感悟有很多：一如，当我第一次接触这些陌生的东西时，我应该多自己查找百度去了解它，而不是畏惧它，现在想想，从刚接触时只是简单地跟着学长说的做却从不能很确切知道这一步的目的，到现在的基本能理解前边的每一步的作用并能够独立学习一些东西，所以，陌生的东西并不可怕，只要自己按部就班地来，自己会一点点理解的。一如，要学会向其他人求助并讨论，当我的yml文件配置不成功时，我打开邮箱看到了大家的讨论并试着改了一下，对我帮助很大，同时我找到了肖滢同学，通过与她的讨论，她最终成功地帮我部署成功了，而我 也真正的明白了自己哪里出错了。所以在独立思考后还不能解决的问题，要多问，多讨论，才能最终解决。一如，要多浏览一些有用的网站，可以帮到自己，我在使用markdown时就是看了知乎上大家发的一些学习资源。 所以，只要一步步慢慢来，我相信我可以学到更多的东西！ 我在学习中参考的一些资源 link1 link2 link3 link4]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>Workshop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2016%2F09%2F25%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
